{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Maximum Likelihood Estimation\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/SaajanM/mat422-homework/blob/main/2.4%20Maximum%20Likelihood%20Estimation/mle.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a numpy package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the numpy package\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pyplt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand\\argmax{\\mathop{\\mathrm{arg\\,max}}\n",
    "\\newcommand\\argmin{\\mathop{\\mathrm{arg\\,min}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.4.1 MLE for Random Samples\n",
    "\n",
    "MLE is an effective approach at estimating the parameters of a probability distribution by instead optimizing a function known as the **likelihood function**.\n",
    "\n",
    "**Definition:** Let $X_1,\\dots,X_n$ have a joint pmf or pdf\n",
    "$$\n",
    "f(x_1,\\dots,x_n;\\theta_1,\\dots,\\theta_m)\n",
    "$$\n",
    "where $\\theta_1,\\dots,\\theta_m$ are called **parameters** and have unknown value. When the $x_i$'s are observed sample values and $f$ is treated as a function of the $\\theta_i$'s, we call $f$ the **likelihood function**. When they are random variables, then we call it a **maximum liklihood estimator**.\n",
    "\n",
    "Given a likelihood function, we can find the **maximum likelihood estimate** by maximizing it across all parameters.\n",
    "\n",
    "Usually, this involves taking partial derivatives and then finding the maxima of the function.\n",
    "\n",
    "Below is code showing that the derived MLE for a normal distribution's mean is close to the real mean of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the distribution mean (5.0) and the observed MLE mean (4.999869230111917) similar?: Yes\n"
     ]
    }
   ],
   "source": [
    "theory_mean = 5.0\n",
    "data_set = np.random.normal(theory_mean,1.5,1000000)\n",
    "observed_mean = data_set.sum() / data_set.size\n",
    "\n",
    "ans = \"Yes\" if np.allclose(theory_mean,observed_mean, atol=0.02) else \"No\"\n",
    "\n",
    "print(\"Are the distribution mean ({}) and the observed MLE mean ({}) similar?: {}\".format(theory_mean,observed_mean,ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.4.2 Linear Regression\n",
    "\n",
    "The traditional way of performing linear regression is thorugh least squares. But now we are equipped with looking at it through the lens of probability.\n",
    "\n",
    "First we define our $n$ data points as $\\{(\\mathbf{x}_i,y_i)\\}_{i=1}^n$.\n",
    "\n",
    "We wish to find $\\beta_0,\\dots,\\beta_n$ such that $\\hat{y}_i = \\beta_0 + \\beta_1\\cdot x_{i1} + \\dots \\beta_n\\cdot x_{in}$ is as close to $y_i$ in the least squares way.\n",
    "\n",
    "We first suppose that our $n$ data points are drawn randomly from a normal distribution in an i.i.d. manner. The joint probability function for a given $\\mu,\\sigma^2$ (not necessarily constants) is:\n",
    "$$\n",
    "\\mathscr{P}(\\mu\\mid y) = \\prod_{i=1}^n P_Y(y_i\\mid \\mu,\\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(y_i-\\mu)^2}{2\\sigma^2}}\n",
    "$$\n",
    "This is because when i.i.d. the joint pdf is equivalent to the multiplication of the input pdfs (the $n$ normal distributions with shared $\\mu,\\sigma^2$).\n",
    "\n",
    "Notice now that $y_i = \\hat{y}_i + \\varepsilon$ where $\\varepsilon$ is drawn from a normal distribution with mean zero and variance $\\sigma^2$. That is to say that we can split up the normally distributed $y_i$ into a sum of a linear function with respect to $\\mathbf{x}_i$ and a normally distributed $\\varepsilon$.\n",
    "\n",
    "Interpreting the $\\hat{y}_i$ as a shift to the right, we can see that $\\mu$ for $y_i$ is nothing but $\\hat{y}_i$. That is to say $y_i$ is a normally distributed variable with mean $\\hat{y}_i$ and variance $\\sigma^2$.\n",
    "\n",
    "Thus our parameters to optimize on for the likelihood function $\\mathscr{P}$ are $\\beta_0, \\dots, \\beta_n, \\sigma^2$. Luckily we only care about $\\beta$ as a vector of all the $\\beta_i$ and can treat it as such. The $\\sigma^2$ we can disregard because it is unimportant for determining $\\beta$.\n",
    "\n",
    "Thus we end up with the optimization problem\n",
    "$$\n",
    "\\hat{\\beta} = \\argmax\\mathscr{P}(\\beta\\mid y) = \\argmax_\\beta \\prod_{i=1}^n \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(y_i-\\hat{y}_i)^2}{2\\sigma^2}}\n",
    "$$\n",
    "Then, because the natural logarithm is monotonically increasing, we can insert it into the arg max without effect.\n",
    "$$\n",
    "\\hat{\\beta} = \\argmax_\\beta \\log\\left(\\prod_{i=1}^n \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(y_i-\\hat{y}_i)^2}{2\\sigma^2}}\\right)\n",
    "$$\n",
    "Applying logarithm rules yields\n",
    "$$\n",
    "\\hat{\\beta} = \\argmax_\\beta \\sum_{i=1}^n \\log\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right) + \\log\\left(e^{-\\frac{(y_i-\\hat{y}_i)^2}{2\\sigma^2}}\\right)\n",
    "$$\n",
    "The first argument is a positive constant and will not impact the argmax result, so we can discard it. We can then cancel the log and the exponential. This gets us\n",
    "$$\n",
    "\\hat{\\beta} = \\argmax_\\beta \\sum_{i=1}^n-\\frac{(y_i-\\hat{y}_i)^2}{2\\sigma^2}\n",
    "$$\n",
    "The bottom is a positive constant scalar, so we can disregard that.\n",
    "$$\n",
    "\\hat{\\beta} = \\argmax_\\beta \\sum_{i=1}^n-(y_i-\\hat{y}_i)^2\n",
    "$$\n",
    "Finally, we can convert this into a minimzation because finding the max of a bunch of negatives is the same as finding the min of the positives.\n",
    "$$\n",
    "\\hat{\\beta} = \\argmin_\\beta \\sum_{i=1}^n(y_i-\\hat{y}_i)^2\n",
    "$$\n",
    "This is exactly the least squares problem and shows how MLE is a marvelous statistical tool."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
