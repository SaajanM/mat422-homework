{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 K Means & 3.6 Support Vector Machines\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/SaajanM/mat422-homework/blob/main/3.5%20K%20Means%20-%203.6%20Support%20Vector%20Machines/kmeans_and_svms.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a numpy package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install scipy\n",
    "!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the numpy package\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pyplt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}$\n",
    "$\\newcommand\\argmax{\\text{arg}\\,\\text{max}}$\n",
    "$\\newcommand\\argmin{\\text{arg}\\,\\text{min}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.5 K-Means\n",
    "\n",
    "$k$-means is a clustering algorithm that partitions $n$ observations into $k$ distinct clusters, while minimizing the squared euclidean distances within the clusters.\n",
    "\n",
    "To be precise, given a data set $\\{\\mathbf{x}_1,\\dots,\\mathbf{x}_n\\}$ where each $\\mathbf{x}_i$ is a $d$-dimensional vector, $k$-means aims to partition the $n$ observations into $k\\leq n$ sets $S=\\{S_1,\\dots,S_k\\}$ so as to minimize the within-cluster sum of squares (WCSS).\n",
    "\n",
    "We define $\\text{WCSS}_i = \\sum_{x\\in S_i} \\norm{x-\\mu(S_i)}^2 and $\\mu(S_i)$ the mean of the points in $S_i$.\n",
    "\n",
    "We therefore have the optimization objective:\n",
    "$$\n",
    "\\argmin_S \\sum_{i=1}^{k} \\text{WCSS}_i\n",
    "$$\n",
    "\n",
    "The algorithm is as follows:\n",
    "\n",
    "1. Select $k$ observations as initial centroids and associate it with a unique cluster\n",
    "2. For each observation, assign it to the cluster of the centroid to which it is closest\n",
    "3. For each cluster, calculate and update the centroids (need not be actual observations).\n",
    "4. Repeat steps 2 and 3 until convergence\n",
    "5. Return resulting partition set.\n",
    "\n",
    "Because the error of $\\text{WCSS}_i$ decreases both in the observation reassignment (2) and centroid update steps (3) this converges to some local minimum.\n",
    "\n",
    "The $k$-means algorithm is available as `scipy`'s `scipy.cluster.vq.kmeans2` function.\n",
    "\n",
    "## Section 3.6 Support Vector Machines\n",
    "\n",
    "Support-vector machines (SVMs) are a supervised machine learning model that aims to analyze data for classification and regression tasks.\n",
    "\n",
    "SVMs, given a $n$ data points that are assigned to one of two labels, seeks to find the hyperplane that divides the two labeled sets in such a way that the distance of the closest data points to the hyper plane is maximized. Prediction are based on the side of the hyperplane the new data points fall under.\n",
    "\n",
    "To be precise, suppose we have a training dataset of $n$ points of the form\n",
    "$$\n",
    "D = \\{(\\mathbf{x}_1,y_1),\\dots,(\\mathbf{x}_n,y_n)\\}\n",
    "$$\n",
    "where $\\mathbf{x}_i$ is a $p$ dimensional vector and $y_i\\in\\{1,-1\\}$.\n",
    "\n",
    "We define a hyperplane as the set of points that satisfy $\\mathbf{w}^T\\mathbf{x}-b=0$ where $\\mathbf{w}$ is a normal vector to the hyperplane\n",
    "\n",
    "If $D$ is linearly seperable then we can define two hyperplanes that seperate the data so that the distance between them is as large as possible. It is the case that the resulting maximum margin hyperplane lying in the middle. Because the labels are $1,-1$, the two hyperplanes we wish to find can be described by: $\\mathbf{w}^T\\mathbf{x}-b=1$ and $\\mathbf{w}^T\\mathbf{x}-b=-1$ respectively.\n",
    "\n",
    "To seperate the data, we wish that all data points in $D$ are subject to: $\\mathbf{w}^T\\mathbf{x}_i-b \\geq 1$ if $y_i = 1$ and $\\mathbf{w}^T\\mathbf{x}_i-b \\leq -1$ if $y_i=-1$.\n",
    "\n",
    "This can be succinctly written as $y_i(\\mathbf{w}^T\\mathbf{x}_i-b) \\geq 1$ for all $(\\mathbf{x_i},y_i)\\in D$.\n",
    "\n",
    "The optimization criterion is thus:\n",
    "$$\n",
    "\\argmin_{\\mathbf{w},\\mathbf{b}} \\left( \\underbrace{\\lambda\\norm{\\mathbf{w}}^2}_{\\text{regularizer}} + \\underbrace{\\frac{1}{n}\\sum_{i=1}^n \\max\\{0,1-y_i(\\mathbf{w}^T\\mathbf{x}_i-b)\\}}_{\\text{error term}} \\right)\n",
    "$$\n",
    "where $\\lambda$ is the regularization parameter responsible for determining allowable amount of misclassification. Smaller values of $\\lambda$ decrease misclassification, but increase the zone in which the SVM is not confident in its result. Larger values do the opposite.\n",
    "\n",
    "This can be solved via stochastic gradient descent methods, yielding the following iteration rules (for a step size $\\beta$ and stochastic datapoint $(\\mathbf{x}_i,y_i)$):\n",
    "$$\n",
    "\\begin{align*}\n",
    "b_{k+1} &= b_k - \\beta\\begin{cases}\n",
    "y_i & \\text{if } 1-y_i(\\mathbf{w_k}^T\\mathbf{x}_i-b_k)\\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\\\\\n",
    "\\mathbf{w}_{k+1} &= \\mathbf{w}_k - \\beta\\begin{cases}\n",
    "2\\lambda\\mathbf{w}_k-\\frac{1}{n}y_i\\mathbf{x}_i & \\text{if } 1-y_i(\\mathbf{w_k}^T\\mathbf{x}_i-b_k)\\\\\n",
    "2\\lambda\\mathbf{w}_k & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mat422",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
