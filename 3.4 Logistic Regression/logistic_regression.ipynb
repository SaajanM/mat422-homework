{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Logistic Regression\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/SaajanM/mat422-homework/blob/main/3.4%20Logistic%20Regression/logistic_regression.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a numpy package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install scipy\n",
    "!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the numpy package\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pyplt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}$\n",
    "$\\newcommand\\argmax{\\text{arg}\\,\\text{max}}$\n",
    "$\\newcommand\\argmin{\\text{arg}\\,\\text{min}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.4.0 Logistic Regression\n",
    "\n",
    "In its basic form, logistic regression is a method of modelling a binary dependent variable via a logistic function. It can be extended beyond this, but the following will pertain to gradient descent on binary classification only.\n",
    "\n",
    "Suppose we have input data in the form $\\left\\{(\\mathbf{\\alpha}_i, \\beta_i:i=1,\\dots,n\\right\\}$ with $\\mathbf{\\alpha}_i\\in\\mathbb{R}^d$ (feature vectors) and $\\beta$ a label of 0 or 1. We use the matrix representation of the two to assist in the math. Namely we let $A$ be the matrix whose $j$th row is the vector $\\mathbf{\\alpha}_j^T$ and let $\\mathbf{b}$ be tho column vector of all the $\\beta_i$.\n",
    "\n",
    "We aim to find a logit function $f$ which predicts the probability of a label of 1. For this purpose, we model the logit function of the probability of label 1 as a linear function of the features.\n",
    "\n",
    "We define our logit function relation to be:\n",
    "$$\n",
    "\\log\\frac{p(\\mathbf{\\alpha};\\mathbf{x})}{1-p(\\mathbf{\\alpha};\\mathbf{x})} = \\mathbf{\\alpha}^T\\mathbf{x}\n",
    "$$\n",
    "where we wish to find parameters $\\mathbf{x}$ that satisfy this equation best w.r.t mean squared error\n",
    "\n",
    "With this, we can derive a function $l$ suitable for gradient descent.\n",
    "$$\n",
    "l(\\mathbf{x};A;\\mathbf{b}) = -\\frac{1}{n}\\sum_{i=1}^nb_i\\log(\\sigma(\\mathbf{\\alpha}^T\\mathbf{x})) - \\frac{1}{n}\\sum_{i=1}^n(1-b_i)\\log(\\sigma(\\mathbf{\\alpha}^T\\mathbf{x}))\n",
    "$$\n",
    "where $\\sigma$ is the sigmoid function.\n",
    "\n",
    "We can derive the gradient and hessian from this and perform gradient descent as standard.\n",
    "The update formula is given for $\\mathbf{x}$ and step size $\\beta$:\n",
    "$$\n",
    "\\mathbf{x}^{k+1} = \\mathbf{x}^k +\\beta\\frac{1}{n}\\sum_{i=1}^nb_i\\log(\\sigma(\\mathbf{\\alpha}_i^T\\mathbf{x}^k))\\alpha_i\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mat422",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
