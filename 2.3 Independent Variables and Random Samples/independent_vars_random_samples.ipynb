{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Independent Variables and Random Samples\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/SaajanM/mat422-homework/blob/main/2.3%20Independent%20Variables%20and%20Random%20Samples/independent_vars_random_samples.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a numpy package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the numpy package\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pyplt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.3.1 Joint Probability Distributions\n",
    "\n",
    "Very rarely in data science do we only care about a single random variable. More often than not we would like to draw conclusions about multiple random variables in how they relate to each other. This is where joint probability distributions come into play.\n",
    "\n",
    "### Section 2.3.1.1 Two Discrete Random Variables\n",
    "\n",
    "We can naturally extend the probability mass function to two variables.\n",
    "\n",
    "**Defintion:** Let $X,Y$ be two discrete random variables defined on the sample space $S$ of an experiment. We define the **joint probability mass function** $p(x,y)$ to be\n",
    "$$\n",
    "p(x,y) = P(X=x\\text{ and } Y=y)\n",
    "$$\n",
    "\n",
    "This definition leads to the conclusion that all values of $p(x,y)$ are greater than zero and the sum of all values over the joint sample space is 1.\n",
    "\n",
    "From the joint distribution, we can extract out information about the original variables using something called a **marginal distribution**. Namely the marginal probability mass function for $X$ is\n",
    "$$\n",
    "p_X(x) = \\sum_{y:p(x,y)>0}p(x,y)\n",
    "$$\n",
    "and respectively for $Y$,\n",
    "$$\n",
    "p_Y(y) = \\sum_{x:p(x,y)>0}p(x,y)\n",
    "$$\n",
    "\n",
    "### Section 2.3.1.2 Two Continuous Random Variables\n",
    "\n",
    "Many of the concepts here extend to continuous space, but with integrals instead of summation. But we should define it.\n",
    "\n",
    "**Definition:** A joint probability density function is any function $f(x,y)$ satisfying $f(x,y)\\geq 0$ for all $x,y$ and $\\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty f(x,y) \\text{d}x\\text{d}y$\n",
    "\n",
    "Then for any two dimensional area $A$, the probabilty of $X,Y$ falling in $A$ is given by\n",
    "$$\n",
    "P((X,Y)\\in A) = \\int\\int_A f(x,y) \\text{d}x\\text{d}y\n",
    "$$\n",
    "\n",
    "Essentially we are finding the volume under the region $A$.\n",
    "\n",
    "The marginal distribution almost is almost perfectly ported over from discrete land, just with integrals.\n",
    "\n",
    "## Section 2.3.1.3 Independent Random Variables\n",
    "\n",
    "This is very similar to the concept of independent random events, but with probability density/mass functions.\n",
    "\n",
    "**Definition:** Two random variables $X,Y$ are said to be independent if for every pair of $x,y$ values\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\textbf{(discrete)} & & p(x,y) = p_X(x) \\cdot p_Y(y)\\\\\n",
    "& \\text{or} & \\\\\n",
    "\\textbf{(continuous)} & & f(x,y) = f_X(x) \\cdot f_Y(y)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The below code demonstrates two discrete random variables that are independent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent?: True\n"
     ]
    }
   ],
   "source": [
    "d20_set_one = np.random.choice(20, 10000)\n",
    "d20_set_two = np.random.choice(20, 10000)\n",
    "data = np.hstack((np.atleast_2d(d20_set_one).T, np.atleast_2d(d20_set_two).T))\n",
    "\n",
    "\n",
    "def joint(x, y):\n",
    "    mask_one = data[:, 0] == x\n",
    "    masked_data = data[mask_one]\n",
    "    mask_two = masked_data[:, 1] == y\n",
    "    res = masked_data[mask_two]\n",
    "    return res.shape[0] / 100000\n",
    "\n",
    "\n",
    "def marginal_x(x):\n",
    "    res = 0\n",
    "    for y in range(20):\n",
    "        res += joint(x, y)\n",
    "    return res\n",
    "\n",
    "\n",
    "def marginal_y(y):\n",
    "    res = 0\n",
    "    for x in range(20):\n",
    "        res += joint(x, y)\n",
    "    return res\n",
    "\n",
    "\n",
    "independent = True\n",
    "\n",
    "for x in range(20):\n",
    "    for y in range(20):\n",
    "        independent = independent and np.allclose(\n",
    "            joint(x, y), marginal_x(x) * marginal_y(y), atol=0.001\n",
    "        )\n",
    "\n",
    "print(\"Independent?: {}\".format(independent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.3.2 Correlation and Dependence\n",
    "\n",
    "Correlations are useful because they can indicate a predictive relationship\n",
    "that can be exploited in practice. Covariance is a measure of the joint vari-\n",
    "ability of two random variables.\n",
    "\n",
    "### Section 2.3.2.1 Correlation for Random Variables\n",
    "\n",
    "In the case of two variables $X,Y$ not independent, we often wish to see how much the two are related.\n",
    "\n",
    "In the following section, we only deal with discrete random variables, but keep in mind that the majority of the concepts directly translate to continuous random variables via use of integrals over summation.\n",
    "\n",
    "**Definition:** The **covariance**, or measure of correlation of $X,Y$ is\n",
    "$$\n",
    "\\text{Cov}(X,Y) = E[(X-\\mu_x)(Y-\\mu_y)]\n",
    "$$\n",
    "\n",
    "Notice that $\\text{Cov}(X,X)=V(X)$.\n",
    "\n",
    "**Definition:** The **correlation coeficient** of $X$ and $Y$, denoted by $\\text{Corr}(X,Y), $\\rho_{X,Y}$, or just $\\rho$, is defined by\n",
    "$$\n",
    "\\rho_{X,Y} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X\\sigma_Y}\n",
    "$$\n",
    "\n",
    "Note that if $X,Y$ are independent, then $\\rho=0$ but the reverse is not true.\n",
    "Also note that this is a linear correlation measure. Not much else.\n",
    "\n",
    "$\\rho$ is also bounded by $[-1,1]$ if $Y$ is a linear combination of $X$.\n",
    "\n",
    "Below is code for calculating the correlation coefficient in `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corellation Coefficient (expected close to 1): 0.9997595641567226\n"
     ]
    }
   ],
   "source": [
    "x = np.linspace(1, 100, num=100)\n",
    "y = x + 2 * np.random.rand(100)\n",
    "\n",
    "print(\n",
    "    \"Corellation Coefficient (expected close to 1): {}\".format(np.corrcoef(x, y)[0, 1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.3.2.2 Correlation For Samples\n",
    "\n",
    "When applied to samples, the correlation coefficient is often written as $r_{xy}$ or $r$.\n",
    "For paired data $\\{(x_1,y_1),\\dots,(x_n,y_n)\\}$ we find that $r_{xy} = \\frac{s_{xy}}{s_x s_y}$ where the sample covariance\n",
    "$$\n",
    "s_xy = \\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\overline{x})(y_i-\\overline{y})\n",
    "$$\n",
    "and the sample standard deviation\n",
    "$$\n",
    "s_x = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\overline{x})^2}\n",
    "$$\n",
    "\n",
    "## Section 2.3.3.1 Random Samples\n",
    "\n",
    "A **simple random sample** is a randomly selected subset of a population and often is used in practice.\n",
    "\n",
    "This is because we can rarely obtain complete information about a population\n",
    "\n",
    "**Definition:** The random variables $X_1,\\dots, X_n$ are said to form a random sample of size $n$ if all $X_i$ are independent random variables and they all share the same probability distribution.\n",
    "\n",
    "We also see that the sample random mean $E(\\overline X)$ is equivalent to the population mean while the sample variance $V(\\overline X)$ is the population variance divided by $n$.\n",
    "\n",
    "### Section 2.3.3.2 Central Limit Theorem\n",
    "\n",
    "Given the random variables $X_1,\\dots, X_n$, the central limit theorem says that as $n$ increases $\\overline X$ approaches a random normal distribution with mean equal to the sample mean and variance equal to the standard variance.\n",
    "\n",
    "The approximation gets better as $n$ increases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mat422",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
