{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Linear Regression\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/SaajanM/mat422-homework/blob/main/1.4%20Principal%20Component%20Analysis/principal_component_analysis.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a numpy package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the numpy package\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pyplt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section covers the basics of principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.0 Principal Component Analysis (PCA) Overview\n",
    "\n",
    "Many problems that data scientists face are very high dimensional. However, oftentimes, we wish to preserve compute power. So we need a way to reduce the dimension of the data without losing too much detail. Principal Component Analysis (PCA) gives us such a method. What it does is to isolate the dimensions that contribute \"the most\" to the data and then we can disregard all \"less important\" dimensions.\n",
    "\n",
    "## 1.4.1 Singular Value Decomposition\n",
    "\n",
    "Let $A$ be an $m\\times n$ matrix. Then $A^T A$ is symmetric and square and can be orthogonally diagonalized. Let $\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}$ be an orthonormal basis for $\\mathbb{R}^n$ consisting of the eigenvectors of $A^TA$, and let $\\lambda_1,\\dots,\\lambda_n$ be the associated eigenvalues of $A^TA$. Then for $1\\leq i\\leq n$,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\norm{A\\mathbf{v}_i}^2 &= (A\\mathbf{v}_i)^TA\\mathbf{v}_i\\\\\n",
    "                       &= \\mathbf{v}_i^T A^T A\\mathbf{v}_i\\\\\n",
    "                       &= \\mathbf{v}_i^T(\\lambda_i\\mathbf{v}_i) & \\text{as $\\mathbf{v}_i$ is an eigenvector of $A^TA$}\\\\\n",
    "                       &= (\\mathbf{v}_i^T \\mathbf{v}_i)\\lambda_i & \\text{commutativity of scalar multiplication over matrix multiplication}\\\\\n",
    "                       &= \\lambda_i & \\text{inner product of two unit vectors is 1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This necessarily means that all eigenvalues are positive (recall that $x^2$ must be positive for real $x$ and the norm is certainly real).\n",
    "\n",
    "Then we can simply assume (by relabeling) that $\\lambda_1\\geq\\lambda_2\\geq\\dots\\geq\\lambda_n\\geq 0$.\n",
    "\n",
    "**Definition:** We call the square roots of the eigenvalues of $A^TA$ above **singular values of $\\mathbf{A}$** (denoted $\\sigma_i$ for a given $\\lambda_i$). They are assumed to be in decreasing order as described above.\n",
    "\n",
    "We assert without proof that for a given $m\\times n$ matrix with $r$ nonzero singular values that $\\text{dim}(\\text{col}(A)) = r$ (this necessitates that all other singular values are zero).\n",
    "\n",
    "Any factorization $A=U\\Sigma V^T$ is called a Singular Value Decomposition (SVD) of an $m\\times n$ matrix $A$ given that $U$ is an $m\\times m$ orthogonal matrix, $V$ is an $n\\times n$ orthogonal matrix, and $Sigma$ is an $m\\times n$ of all zeros save the main diagonal -- whose entries are the singular values of $A$.\n",
    "\n",
    "We can compute such a decomposition of an $m\\times n$ matrix $A$ as follows:\n",
    "1. Obtain the $n$ eigenvalues $\\lambda_i$ and $n$ eigenvectors $\\mathbf{v}_i$ of $A^TA$\n",
    "2. Calculate the $r$ nonzero singular values $\\sigma_i$ in decreasing order.\n",
    "3. Define $\\mathbf{u}_i=\\frac{1}{\\sigma_i}A\\mathbf{v}_i$ for $1\\leq i\\leq r$.\n",
    "4. Extend the orthonormal basis for $\\mathbb{R}^r$ $\\mathbf{u}_1,\\dots,\\mathbf{u_r}$ into an orthonormal basis for $\\mathbb{R}^m$ by appending unit vectors of higher dimension.\n",
    "5. Then let the matrix $U = [\\mathbf{u}_1\\text{ }\\dots\\text{ }\\mathbf{u}_m]$\n",
    "6. Next, let the matrix $V= [\\mathbf{v}_1\\text{ }\\dots\\text{ }\\mathbf{v}_n]$\n",
    "7. Finally let $\\Sigma$ me an $m\\times n$ matrix with the nonzero singular values on the main diagonal.\n",
    "8. The SVD is $A=U\\Sigma V^T$\n",
    "\n",
    "A rudimentary python implementation follows (old method implementations from previous assignments are replaced by their more performant `numpy` counterparts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[1 4]\n",
      " [2 7]\n",
      " [3 1]]\n",
      "Matrix U:\n",
      "[[-0.47902834  0.1520521   0.        ]\n",
      " [-0.84802141  0.17418982  0.        ]\n",
      " [-0.22669702 -0.97290188  1.        ]]\n",
      "Matrix Sigma:\n",
      "[[8.56863758 0.        ]\n",
      " [0.         2.56484894]\n",
      " [0.         0.        ]]\n",
      "Transposed Matrix V:\n",
      "[[-0.33321076 -0.94285237]\n",
      " [-0.94285237  0.33321076]]\n",
      "SVD Product:\n",
      "[[1. 4.]\n",
      " [2. 7.]\n",
      " [3. 1.]]\n",
      "Is the SVD Product equal to A?: Yes\n"
     ]
    }
   ],
   "source": [
    "def svd(A):\n",
    "    prod = A.T @ A  # n * n matrix\n",
    "    eig = np.linalg.eig(prod)  # n eigenvalues and n eigenvectors\n",
    "    eig_pairs = [\n",
    "        (eig.eigenvalues[i], eig.eigenvectors.T[i])\n",
    "        for i in range(eig.eigenvalues.shape[0])\n",
    "    ]\n",
    "    eig_pairs.sort(key=lambda v: -v[0])  # sort them descending\n",
    "    for pair in eig_pairs:\n",
    "        assert np.allclose(prod @ pair[1], pair[0] * pair[1])\n",
    "    V = np.array(list(map(lambda v: v[1], eig_pairs)))  # get eigen vector matrix\n",
    "\n",
    "    nz_eig_pairs = filter(lambda x: not np.allclose(x[0], 0), eig_pairs)\n",
    "    nz_singles = np.array(list(map(lambda x: math.sqrt(x[0]), nz_eig_pairs)))\n",
    "\n",
    "    # Calculate the U matrix and expand as needed\n",
    "    U_small = np.array(\n",
    "        [(1 / (nz_singles[i])) * (A @ V[i]) for i in range(nz_singles.shape[0])]\n",
    "    )\n",
    "    missing_vecs = max(U_small.shape) - min(U_small.shape)\n",
    "    pad_vec_u = [(0, missing_vecs), (0, 0)]\n",
    "    pad_vec_ident = [(0, missing_vecs), (0, missing_vecs)]\n",
    "    U = np.pad(U_small, pad_vec_u, mode=\"constant\") + (\n",
    "        np.identity(max(U_small.shape))\n",
    "        - np.pad(np.identity(min(U_small.shape)), pad_vec_ident, mode=\"constant\")\n",
    "    )\n",
    "\n",
    "    # Calculate Sigma\n",
    "    Sigma_small = np.diag(nz_singles)\n",
    "    pad_vec_sigma = [\n",
    "        (0, A.shape[0] - nz_singles.shape[0]),\n",
    "        (0, A.shape[1] - nz_singles.shape[0]),\n",
    "    ]\n",
    "    Sigma = np.pad(Sigma_small, pad_vec_sigma, mode=\"constant\")\n",
    "\n",
    "    return (U.T, Sigma, V.T)\n",
    "\n",
    "\n",
    "# A = np.array([[1, 2, 3], [4, 7, 1]])\n",
    "A = np.array([[1, 4], [2, 7], [3, 1]])\n",
    "print(\"Matrix A:\\n{}\".format(A))\n",
    "(U, Sigma, V) = svd(A)\n",
    "print(\"Matrix U:\\n{}\".format(U))\n",
    "print(\"Matrix Sigma:\\n{}\".format(Sigma))\n",
    "print(\"Transposed Matrix V:\\n{}\".format(V.T))\n",
    "product = U @ Sigma @ V.T\n",
    "print(\"SVD Product:\\n{}\".format(product))\n",
    "print(\n",
    "    \"Is the SVD Product equal to A?: {}\".format(\n",
    "        \"Yes\" if np.allclose(A, product) else \"No\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.4.2 Low Rank Matrix Approximations\n",
    "\n",
    "Before we begin a discussion of approximating matrices, we first need to know what it means for matrices to be \"close\".\n",
    "\n",
    "**Definition: (Induced Norm)** The 2-norm of a matrix $A\\in\\mathbb{R}^{n\\times m}$ is\n",
    "$$\n",
    "\\norm{A}_2 = \\max_{\\mathbf{0}\\neq\\mathbf{x}\\in\\mathbb{R}^m}\\frac{\\norm{A\\mathbf{x}}}{\\norm{\\mathbf{x}}}\n",
    "           = \\max_{\\mathbf{0}\\neq\\mathbf{x}\\in\\mathbb{R}^m, \\norm{\\mathbf{x}}=1} \\norm{A\\mathbf{x}}\n",
    "           = \\max_{\\mathbf{0}\\neq\\mathbf{x}\\in\\mathbb{R}^m, \\norm{\\mathbf{x}}=1} \\mathbf{x}^TA^TA\\mathbf{x}\n",
    "$$\n",
    "\n",
    "Now, we can discuss what happens when we modify our SVDs.\n",
    "\n",
    "Let $A\\in\\mathbb{R}^{n\\times m}$ have an SVD expressed as\n",
    "$$\n",
    "    A = \\sum_{j=1}^{r}\\sigma_j\\mathbf{u}_j\\mathbf{v}_j^T\n",
    "$$\n",
    "\n",
    "For $k < r$ we let\n",
    "$$\n",
    "    A_k = \\sum_{j=1}^{k}\\sigma_j\\mathbf{u}_j\\mathbf{v}_j^T\n",
    "$$\n",
    "\n",
    "We call $A_k$ a $k$-rank approximation of $A$. This is essentially done by restricting which singular values to use. Namely, the $r-k$ smallest singular values are discarded.\n",
    "\n",
    "We can show that\n",
    "$$\n",
    "\\norm{A-A_k}_2^2 = \\sigma_{k+1}^2\n",
    "$$\n",
    "\n",
    "It is also known that for any rank $k$ (dimension of column space) matrix $B$, the $k-rank$ approximation defined above is the best approximation with regards to the induced norm.\n",
    "\n",
    "A sample implementation is defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[1 4]\n",
      " [2 7]\n",
      " [3 1]]\n",
      "Rank 1 Approximation:\n",
      "[[1.36770362 3.87005091]\n",
      " [2.42123869 6.85113124]\n",
      " [0.64725678 1.83147625]]\n",
      "Rank 2 Singular Value Squared: 6.578450065863193\n",
      "Induced Error Norm Squared(||A-A_k||_2^2): 6.578450065863193\n",
      "Is the square of the rank 2 singular value equal to the square of the normed error?: Yes\n"
     ]
    }
   ],
   "source": [
    "def k_rank_approx(A, k):\n",
    "    (U, Sigma, V) = svd(A)\n",
    "    singulars = np.diag(Sigma)\n",
    "    rank_k_singulars = singulars[0:k]\n",
    "    Sigma_small = np.diag(rank_k_singulars)\n",
    "    pad_vec_sigma = [\n",
    "        (0, A.shape[0] - rank_k_singulars.shape[0]),\n",
    "        (0, A.shape[1] - rank_k_singulars.shape[0]),\n",
    "    ]\n",
    "    Sigma = np.pad(Sigma_small, pad_vec_sigma, mode=\"constant\")\n",
    "    return U @ Sigma @ V.T\n",
    "\n",
    "\n",
    "A = np.array([[1, 4], [2, 7], [3, 1]])\n",
    "print(\"Matrix A:\\n{}\".format(A))\n",
    "approx = k_rank_approx(A, 1)\n",
    "print(\"Rank 1 Approximation:\\n{}\".format(approx))\n",
    "(_, Sigma, _) = svd(A)\n",
    "rank_2_val_squared = np.diag(Sigma)[-1] ** 2\n",
    "print(\"Rank 2 Singular Value Squared: {}\".format(rank_2_val_squared))\n",
    "squared_error = np.linalg.norm(A - approx) ** 2\n",
    "print(\"Induced Error Norm Squared(||A-A_k||_2^2): {}\".format(squared_error))\n",
    "print(\n",
    "    \"Is the square of the rank 2 singular value equal to the square of the normed error?: {}\".format(\n",
    "        \"Yes\" if np.allclose(squared_error, rank_2_val_squared) else \"No\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.4.3 Principal Component Analysis\n",
    "\n",
    "## Section 1.4.3.1 Covariance Matrix\n",
    "\n",
    "Before we begin discussing the way PCA works and how to implement, we first need to describe what is known as the **Covariance Matrix**.\n",
    "\n",
    "Let $[\\mathbf{X}_1\\dots\\mathbf{X}_N]$ be a $p\\times N$ matrix called the **matrix of observation**. The **sample mean** of the observation matrix vectors $\\mathbf{X}_i$ is given by\n",
    "$$\n",
    "\\mathbf{M} = \\frac{1}{N}(\\mathbf{X}_1+\\dots+\\mathbf{X}_N)\n",
    "$$\n",
    "For $k = 1\\dots N$, let $\\hat{\\mathbf{X}}_k=\\mathbf{X}_k - \\mathbf{M}$. With this, we can construct a matrix $B = [\\hat{\\mathbf{X}_1}\\dots\\hat{\\mathbf{X}_N}]$. The columns of $B$ have zero sample mean and we say $B$ is in **mean-deviation** form.\n",
    "\n",
    "The **sample covariance matrix** of $A$ is a $p\\times p$ matrix defined below.\n",
    "$$\n",
    "S = \\frac{1}{N-1}BB^T\n",
    "$$\n",
    "Since any matrix of the form $BB^T$ is positive semidefinite, so is $S$. Recall, positive semidefinite just means that for every nonzero column of $S$ (denoted $z$ here) the number $z^TSz$ is non-negative.\n",
    "\n",
    "A covariance matrix implementation is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[1 4]\n",
      " [2 7]\n",
      " [3 1]]\n",
      "Matrix A in Mean Deviation Form:\n",
      "[[-1.  0.]\n",
      " [ 0.  3.]\n",
      " [ 1. -3.]]\n",
      "Is rows of MD Form Orthogonal?: Yes\n"
     ]
    }
   ],
   "source": [
    "def mean_dev_form(A):\n",
    "    return A - np.mean(A, axis=0)\n",
    "\n",
    "\n",
    "A = np.array([[1, 4], [2, 7], [3, 1]])\n",
    "print(\"Matrix A:\\n{}\".format(A))\n",
    "mdform = mean_dev_form(A)\n",
    "print(\"Matrix A in Mean Deviation Form:\\n{}\".format(mdform))\n",
    "print(\n",
    "    \"Is rows of MD Form Orthogonal?: {}\".format(\n",
    "        \"Yes\" if np.dot(mdform[0], mdform[1]) == 0 else \"No\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.4.3.2 Principal Component Analysis\n",
    "\n",
    "We now assume that the $p\\times N$ data matrix $X=[\\mathbf{X}_1\\dots\\mathbf{X}_N]$ is already in mean deviation form.\n",
    "\n",
    "The goal of PCA is to find $k$ (where $k\\leq p$) orthonormal vectors $\\mathbf{v}_1,\\dots,\\mathbf{v}_k$ (known as the **top $\\mathbf{k}$ principal components of $\\mathbf{X}$**) that maximize\n",
    "$$\n",
    "\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^k \\langle\\mathbf{X}_i\\cdot \\mathbf{v}_j\\rangle^2\n",
    "$$\n",
    "where $\\langle\\mathbf{X}_i\\cdot \\mathbf{v}_j\\rangle$ is the length of the projection of $\\mathbf{X}_i$ onto $\\mathbf{v}_j$.\n",
    "\n",
    "We can see that for each $j$,\n",
    "$$\n",
    "\\mathbf{v}_j^TXX^T\\mathbf{v}_j = (X^T\\mathbf{v}_j)^T(X^T\\mathbf{v}_j) = \\sum_{i=1}^{N}\\langle\\mathbf{X}_i\\cdot \\mathbf{v}_j\\rangle^2\n",
    "$$\n",
    "\n",
    "So we can simplify and rephrase the variance maximization problem for each $j\\leq k$ to\n",
    "$$\n",
    "\\argmax_{\\mathbf{v},\\norm{\\mathbf{v}} = 1} \\mathbf{v}_j^TXX^T\\mathbf{v}_j \n",
    "$$\n",
    "\n",
    "We can orthogonally diagonalize $XX^T$ so that $XX^T=V\\text{diag}(\\lambda_1,\\dots,\\lambda_{p})V^T$. In view of having learned about SVD, we can then conclude that the best choices for the objective function would be the first $k$ eigenvectors corresponding to the first $k$ largest eigenvalues. These can be found in the first $k$ columns of $V$ as obtained through SVD.\n",
    "\n",
    "Then we can perform a change of variables from each $\\mathbf{x}$ to a new $\\mathbf{y}$ given by $\\mathbf{y} = V^T\\mathbf{x}$.\n",
    "\n",
    "To perform dimensionality reduction, simply truncate $V$ to the top $k$ PCA vectors before transposing and performing change of variables. This PCA transform  preserves most of the variance of the original data.\n",
    "\n",
    "The percent of variance captured is given by\n",
    "$$\n",
    "\\frac{\\sum_1^k \\lambda_k}{\\sum_1^p \\lambda_p} \\times 100\\%\n",
    "$$\n",
    "\n",
    "an implementation of dimensionality reduction through PCA is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A in Mean Deviation Form:\n",
      "[[-1.          0.66666667]\n",
      " [ 0.          2.66666667]\n",
      " [ 1.         -3.33333333]]\n",
      "[9.78847487 0.54485846]\n",
      "Variance Kept: 1.0\n",
      "PCA dimension reduced data:\n",
      "[[ 0.98180399 -4.31427986]\n",
      " [ 1.01787078  0.23163764]]\n"
     ]
    }
   ],
   "source": [
    "def dim_reduce(X, k):\n",
    "    (_, Sigma, V) = svd(X.T / np.sqrt(X.shape[1]))\n",
    "    e_vals = np.square(np.diag(Sigma))\n",
    "    print(e_vals)\n",
    "    reduced_evals = e_vals[0:k]\n",
    "    print(\"Variance Kept: {}\".format(np.sum(reduced_evals) / np.sum(e_vals)))\n",
    "    reduced_evecs = V[:, 0:k]\n",
    "    out = np.zeros((k, X.shape[1]))\n",
    "    for i in range(X.shape[1]):\n",
    "        out[:, i] = reduced_evecs.T @ X[:, i]\n",
    "    return out\n",
    "\n",
    "\n",
    "A = np.array([[1, 5], [2, 7], [3, 1]])\n",
    "\n",
    "A = mean_dev_form(A)\n",
    "print(\"Matrix A in Mean Deviation Form:\\n{}\".format(A))\n",
    "res = dim_reduce(A, 2)\n",
    "print(\"PCA dimension reduced data:\\n{}\".format(res))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mat422",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
