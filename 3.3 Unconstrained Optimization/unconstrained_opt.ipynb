{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Unconstrained Optimization\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/SaajanM/mat422-homework/blob/main/3.3%20Unconstrained%20Optimization/unconstrained_opt.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a numpy package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install scipy\n",
    "!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the numpy package\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pyplt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}$\n",
    "$\\newcommand\\argmax{\\text{arg}\\,\\text{max}}$\n",
    "$\\newcommand\\argmin{\\text{arg}\\,\\text{min}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.3.1 Necessary and Suï¬€icent Conditions of Local Minimizers\n",
    "\n",
    "Unconstrained optimization deals with the following optimzation objective:\n",
    "$$\n",
    "\\min_{\\mathbf{x}\\in\\mathbb{R}^d} f(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "where $f$ is a function that maps from the $d$-dimensional reals to the single dimension reals. In this subsection we discuss several definitions of the term \"solution\" as well as derive corresponding characterizations.\n",
    "\n",
    "Ideally we wish to find a global minimizer to the optimization problem above:\n",
    "\n",
    "**Definition:** $\\mathbf{X}^\\ast \\in\\mathbb{R}^d$ is a **global minimizer** over $f$ if:\n",
    "$$\n",
    "f(\\mathbf{x})\\geq f(\\mathbf{x}^\\ast), \\forall{\\mathbf{x}\\in\\mathbb{R}^d}\n",
    "$$\n",
    "\n",
    "Often this is fairly difficult to find numerically unless certain properties of $f$ are presented. So we define other forms of minimization.\n",
    "\n",
    "**Definition:** $\\mathbf{x}^\\ast$ is a **local minimizer** over $f$ if there is an open ball around $\\mathbf{x}^\\ast$ where $\\mathbf{x}^\\ast$ is the minimum value.\n",
    "\n",
    "We will characterize local minimizers in terms of the **gradient** and **Hessian** of the function.\n",
    "\n",
    "But first let us discuss what a **descent direction** is. $\\mathbf{v}$ is a descent direction of $f$ at $\\mathbf{x}_0$ if there is an $\\alpha^\\ast > 0$ such that $f(\\mathbf{x}_0 + \\alpha\\mathbf{v}) < \\mathbf{x}_0$ for all $\\alpha <\\alpha^\\ast$\n",
    "\n",
    "Basically it is a direction where all step sizes up to an upper bound guarantee decreasing the function.\n",
    "\n",
    "It turns out that $\\mathbf{v}$ is a descent direction if the directional derivative in direction $\\mathbf{v}$ is negative.\n",
    "\n",
    "In fact at all points where the gradient is not zero, the function has a descent direction. Namely it is the negation of the gradient vector.\n",
    "\n",
    "The following theorem extends the result that the derivative of a function\n",
    "is zero at a minimizer.\n",
    "\n",
    "**First Order Necessary Condition:** If $\\mathbf{x}_0$ is a local minimizer then the gradient of $f$ at this point is the zero vector.\n",
    "\n",
    "If $f$ is twice continuously differentiable, the Hessian of the function can\n",
    "play an important role.\n",
    "\n",
    "**Definition:** A square symmetrix $d\\times d$ matrix $H$ is **positive-semidefinite** (PSD) if $\\mathbf{x}^TH\\mathbf{x} \\geq 0$ for all $\\mathbf{x}\\in\\mathbb{R}^d$.\n",
    "\n",
    "**Second Order Necessary Condition:** If $\\mathbf{x}_0$ is a local minimizer, then the hessian (if it exists) is PSD.\n",
    "\n",
    "**Second Order Sufficient Condition:** If, at a point $\\mathbf{x}_0$, $f$ has a gradient of zero and the Hessian is positive definite, then this point is a strict local minimizer.\n",
    "\n",
    "## Section 3.3.2 Convexity and Global Minimizers\n",
    "\n",
    "Here we only consider convex functions. This means the line segment between any two points on the graph of the function lies above the function. This type of function has the great benefit of making all local minima into global minima.\n",
    "\n",
    "**Definition:** A set $D\\subseteq\\mathbb{R}^d$ is a **convex set** if for all $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^d$ and all $\\alpha\\in[0,1]$, we have that $(1-\\alpha)\\mathbf{x}+\\alpha\\mathbf{y}\\in D$.\n",
    "\n",
    "We can extend this to **convex functions**. That is if we linearly interpolate over function arguments, we result in a value no more than the linear interpolation of the function outputs.\n",
    "\n",
    "It turns out that all affine functions are convex.\n",
    "\n",
    "We can prove other types of functions are convex by looking at their Hessian. But we start with the first order condition.\n",
    "\n",
    "**First-Order Convexity Condition** $f$ is convex if and only if for all $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^d$,\n",
    "$$\n",
    "    f(\\mathbf{y})\\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T(\\mathbf{y}-\\mathbf{x})\n",
    "$$\n",
    "\n",
    "We can build on this to get the **second order convexity condition**: The function is convex if at all points the hessian is PSD\n",
    "\n",
    "Due to these conditions, then we know that all that is sufficient and necessary to be a global minimizer is to have a gradient of zero at that point.\n",
    "\n",
    "### 3.3.3 Gradient Descent\n",
    "\n",
    "**Gradient descent** is an iterative optimization algorithm for finding a local minimum of a differentiable function.\n",
    "\n",
    "Because most functions are nonlinear and therefore cannot have their stationary points easily found.\n",
    "\n",
    "Essentially, all gradient descent (particularly **steepest descent**) is doing is taking steps in the direction of the negative gradient. The size of the $k$-th step are determined by:\n",
    "$$\n",
    "\\argmin_{\\alpha > 0} f(\\mathbf{x}^k) - \\alpha\\nabla f(\\mathbf{x}^k)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mat422",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
