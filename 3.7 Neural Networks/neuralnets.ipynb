{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.7 Neural Networks\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/SaajanM/mat422-homework/blob/main/3.7%20Neural%20Networks/neuralnets.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a numpy package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install scipy\n",
    "!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the numpy package\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pyplt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}$\n",
    "$\\newcommand\\argmax{\\text{arg}\\,\\text{max}}$\n",
    "$\\newcommand\\argmin{\\text{arg}\\,\\text{min}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.7.0 Neural Networks\n",
    "\n",
    "Based on the human brain, artificial neural networks - aka neural networks - are multilinear and sometimes nonlinear systems that transform some input vector into an output vector after some number of transformations (layers).\n",
    "\n",
    "## Section 3.7.1 Mathematical Formulation\n",
    "\n",
    "We will mainly talk about classical feed-forward neural networks here, where multiple layers are present with no backtracking or special layer types.\n",
    "\n",
    "Here we will discuss how the values (activations) of layer $l$ are calculated using the activations of layer $l-1$ ($\\mathbf{a}^{(l-1)}$), the biases of layer $l$ ($\\mathbf{b}^{(l)}$), and the weights of layer $l$ ($W^{(l)}$).\n",
    "\n",
    "Because the pre-activation function values of the layer $l$ ($\\mathbf{z}^l$) are an affine function of those in layer $l-1$ we can write\n",
    "$$\n",
    "\\mathbf{z}^{(l)} = W^{(l)}\\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)}\n",
    "$$\n",
    "\n",
    "Finally we apply some transform (the activation function $sigma$) to the pre-function activations to get\n",
    "\n",
    "$$\n",
    "\\mathbf{a}^{(l)} = \\sigma(\\mathbf{z}^{(l)})\n",
    "$$\n",
    "\n",
    "## Section 3.7.2 Activation Functions\n",
    "\n",
    "We just talked about activation functions but what even are they?\n",
    "\n",
    "They are functions that perform some transform on the incoming activations. This is usally to apply some mathematical properties to the end result (perhaps boundedness, linearity, etc) or to give an intuitive understanding for what these activations represent (softmax: proportion of total)\n",
    "\n",
    "### Step Function\n",
    "\n",
    "This activation function is takes on a value of zero for all inputs less than 0 and 1 for all other values. It can be used for classification tasks.\n",
    "\n",
    "### Rectified Linear Unit (ReLU)\n",
    "\n",
    "The ReLU is simply $\\max(0,x)$. It represents either leaving a signal untouched or killing it. It is very popular because of its good performance and easiness to compute.\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "This is the function $\\frac{1}{1+e^{-x}}$. It has heavy use in STEM applications and can be used as a final layer.\n",
    "\n",
    "### Softmax\n",
    "\n",
    "This function simply converts the input vector into a vector of weighted probabilities. This is heavily used in classification tasks, because it can represent a soft confidence score rather than a YES/NO answer.\n",
    "\n",
    "## Section 3.7.3 Cost Function\n",
    "\n",
    "In the realm of supervised learning we must have a cost function that lets us know how far the outputs of our network are to the desired outputs. This is either usually the least squares or cross entropy loss functions.\n",
    "\n",
    "We denote this function $J$\n",
    "\n",
    "## Section 3.7.4 Back Propagation\n",
    "\n",
    "Direct gradient descent on nonlinear activation functions across the entire network is wildly expensive, so modern systems employ a method known as backpropagation. It is called backpropagation because we are propagating the error of the last layer backwards through the entire system in order to fine tune the weights and biases. Essentially, we perform gradient descent on a layer-by-layer basis to calculate the overall network gradients\n",
    "\n",
    "Because we care about weights and biases, we are most concerned with calculating the gradients involving the weights and biases from node $j$ in layer $l-1$ to node $j'$ in layer $l$:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w^{(l)}_{j,j'}},\\hspace{3em}\\frac{\\partial J}{\\partial b^{(l)}_{j'}}\n",
    "$$\n",
    "\n",
    "We introduce the quantity $\\delta_{j'}^{(l)} = \\frac{\\partial J}{\\partial z_{j'}^(l)}$\n",
    "\n",
    "We can actually find that this value relies on the many $\\delta_{j'}^{(l+1)}$. Additionally we can find that our original gradients can be calculated from all the delta values. Therefore, if we find the rightmost delta we can backpropagate it through the full network and find the gradients for all the parameters.\n",
    "\n",
    "When dealing with the activation functions, because they are very simple (especially in the case of ReLU) we can use the chain rule.\n",
    "\n",
    "## Section 3.7.5 Back Propagation Algorithm\n",
    "\n",
    "We first initialize the parameters of the network randomly. Then we pick an input vector at random and calculate our networks output. Then we calculate the gradients and update our parameters according to gradient descent. We then repeat until our desired accuracy of the network is reached."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
